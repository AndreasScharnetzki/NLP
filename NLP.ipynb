{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---\n",
    "### <font color=\"red\"><center>A restart might be necessary after the installation.</center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valence Aware Dictionary and sEntiment Reasoner (VADER)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Stopwords\"\"\"\n",
    "# nltk.download(\"<MODULE_NAME>\") e.g. \"stopwords\", \"gutenberg\" or \"wordnet\"\n",
    "# src: https://pypi.org/project/nltk/\n",
    "\"\"\"Number to Word Translalator\"\"\"\n",
    "# pip install num2words\n",
    "# src: https://pypi.org/project/words2num/\n",
    "\"\"\"NLP Model for Lemmatization and Named Entity Recognition (NER)\"\"\"\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# NOTE: If pip is not available to you, please refer to https://spacy.io/usage and select an installation option suiting to your environment :)\n",
    "\"\"\"Cython - needs to be installed to make use of the 'worker' parameter in Word2Vec model (parallelization)\"\"\"\n",
    "# pip install Cython\n",
    "# src: https://pypi.org/project/Cython/\n",
    "\"\"\"Valence Aware Dictionary and sEntiment Reasoner (VADER)\"\"\"\n",
    "# pip install vaderSentiment\n",
    "# src: https://pypi.org/project/vaderSentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and loading the data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords, gutenberg\n",
    "from nltk import ngrams\n",
    "from num2words import num2words\n",
    "from gensim.models import Word2Vec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from IPython.display import display\n",
    "\n",
    "# src: https://wortschatz.uni-leipzig.de/en/download/English#eng_news_2020 (accessed: 14.12.21)\n",
    "with open(\"eng_news_2020_10K-sentences.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    corpus_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the enumeration infront of each doc\n",
    "corpus_without_enum = [doc.split(\"\\t\")[1] for doc in corpus_raw]\n",
    "# converting every word to lower case\n",
    "corpus_lowercase = [doc.lower() for doc in corpus_without_enum]\n",
    "# insert whitespace between numbers and words\n",
    "corpus_words_and_numbers_separated = [re.sub(\"([a-zA-Z]*)(\\d+)([a-zA-Z]*)\", r\"\\1 \\2 \\3\", doc) for doc in corpus_lowercase]\n",
    "# convert numbers into their word equivalent while maintaining their position\n",
    "corpus_num2words = []\n",
    "for doc in corpus_words_and_numbers_separated:\n",
    "    numbers_in_doc = re.findall(\"\\d+\", doc)  \n",
    "    # returns false if list is empty\n",
    "    if numbers_in_doc:\n",
    "        for number in numbers_in_doc:\n",
    "            doc = doc.replace(number, num2words(number, lang=\"en\"))\n",
    "        corpus_num2words.append(doc)    \n",
    "    else:\n",
    "        corpus_num2words.append(doc)   \n",
    "# removing words shorter than 4 characters\n",
    "corpus_longer_words = [re.sub(r\"\\b\\w{,3}\\b\", \"\", doc) for doc in corpus_num2words]\n",
    "# removing punctuation\n",
    "corpus_no_punctuation = [re.sub(r\"(\\W+)\", \" \", doc) for doc in corpus_longer_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization, loading the small standard Model with all components \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Loop is necessary due to spaCys nlp() method does not accept iterables\n",
    "lemmatized_corpus = []\n",
    "for doc in corpus_no_punctuation:\n",
    "    loaded_doc = nlp(doc)\n",
    "    lemmatized_corpus.append(\" \".join([token.lemma_ for token in loaded_doc])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech-Tagging (POS-Tagging)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT: actress political activist longoria praise host ability first night democratic national convention last night \n",
      "\n",
      "TEXT: actress         POS: VERB       DETAILS: verb, base form\n",
      "TEXT: political       POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: activist        POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: longoria        POS: PROPN      DETAILS: noun, proper singular\n",
      "TEXT: praise          POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: host            POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: ability         POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: first           POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: night           POS: PROPN      DETAILS: noun, proper singular\n",
      "TEXT: democratic      POS: PROPN      DETAILS: noun, proper singular\n",
      "TEXT: national        POS: PROPN      DETAILS: noun, proper singular\n",
      "TEXT: convention      POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: last            POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: night           POS: NOUN       DETAILS: noun, singular or mass\n",
      "\n",
      "VISUALIZATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a3cc0865c6fb4fd698acec96188782ab-0\" class=\"displacy\" width=\"2150\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #ECB365; background: #041C32; font-family: Source Sans Pro; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">actress</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">political</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">activist</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">longoria</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">praise</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">host</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">ability</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">first</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">night</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">democratic</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">national</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1700\">convention</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1700\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">last</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2000\">night</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2000\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-0\" stroke-width=\"2px\" d=\"M212,527.0 212,502.0 332.0,502.0 332.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,529.0 L208,521.0 216,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-1\" stroke-width=\"2px\" d=\"M362,527.0 362,452.0 788.0,452.0 788.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,529.0 L358,521.0 366,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-2\" stroke-width=\"2px\" d=\"M512,527.0 512,477.0 785.0,477.0 785.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M512,529.0 L508,521.0 516,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-3\" stroke-width=\"2px\" d=\"M662,527.0 662,502.0 782.0,502.0 782.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,529.0 L658,521.0 666,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-4\" stroke-width=\"2px\" d=\"M812,527.0 812,502.0 932.0,502.0 932.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,529.0 L808,521.0 816,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-5\" stroke-width=\"2px\" d=\"M62,527.0 62,427.0 941.0,427.0 941.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M941.0,529.0 L945.0,521.0 937.0,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-6\" stroke-width=\"2px\" d=\"M1112,527.0 1112,502.0 1232.0,502.0 1232.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1112,529.0 L1108,521.0 1116,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-7\" stroke-width=\"2px\" d=\"M62,527.0 62,402.0 1244.0,402.0 1244.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1244.0,529.0 L1248.0,521.0 1240.0,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-8\" stroke-width=\"2px\" d=\"M1412,527.0 1412,477.0 1685.0,477.0 1685.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1412,529.0 L1408,521.0 1416,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-9\" stroke-width=\"2px\" d=\"M1562,527.0 1562,502.0 1682.0,502.0 1682.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1562,529.0 L1558,521.0 1566,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-10\" stroke-width=\"2px\" d=\"M62,527.0 62,377.0 1697.0,377.0 1697.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1697.0,529.0 L1701.0,521.0 1693.0,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-11\" stroke-width=\"2px\" d=\"M1862,527.0 1862,502.0 1982.0,502.0 1982.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1862,529.0 L1858,521.0 1866,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a3cc0865c6fb4fd698acec96188782ab-0-12\" stroke-width=\"2px\" d=\"M62,527.0 62,352.0 2000.0,352.0 2000.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a3cc0865c6fb4fd698acec96188782ab-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2000.0,529.0 L2004.0,521.0 1996.0,521.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select a random document from corpus\n",
    "random_doc = random.choice(lemmatized_corpus)\n",
    "loaded_doc = nlp(random_doc.strip())\n",
    "\n",
    "print(\"CHOSEN DOCUMENT:\", loaded_doc, \"\\n\")\n",
    "# Use spaCy POS-Tagging\n",
    "for token in loaded_doc:\n",
    "    print(\n",
    "        f\"TEXT: {token.text:15}\",\n",
    "        f\"POS: {token.pos_:10}\", \n",
    "        f\"DETAILS: {spacy.explain(token.tag_):5}\"\n",
    "        )\n",
    "\n",
    "print(\"\\nVISUALIZATION:\")\n",
    "# specify options for visualization        \n",
    "options = {\n",
    "    \"compact\": True, \n",
    "    \"bg\": \"#041C32\",\n",
    "    \"color\": \"#ECB365\", \n",
    "    \"font\": \"Source Sans Pro\"\n",
    "    }\n",
    "# display POS and relationship of words within the sentence    \n",
    "spacy.displacy.render(loaded_doc, style=\"dep\", options=options)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entitiy Recognition (NER)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT: As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect. \n",
      "\n",
      "TEXT: Gregor Samsa    LABEL: PERSON     DETAILS: People, including fictional\n",
      "TEXT: one morning     LABEL: TIME       DETAILS: Times smaller than a day\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">As \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gregor Samsa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " awoke \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one morning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " from uneasy dreams he found himself transformed in his bed into a gigantic insect.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Due to not every doc in the corpus contains entities, a sample is provided manually\n",
    "loaded_doc = nlp(\"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\")\n",
    "\n",
    "print(\"CHOSEN DOCUMENT:\", loaded_doc, \"\\n\")\n",
    "# Use spaCy POS-Tagging\n",
    "for token in loaded_doc.ents:\n",
    "    print(\n",
    "        f\"TEXT: {token.text:15}\",\n",
    "        f\"LABEL: {token.label_:10}\", \n",
    "        f\"DETAILS: {spacy.explain(token.label_):19}\"\n",
    "        )\n",
    "\n",
    "# display POS and relationship of words within the sentence    \n",
    "spacy.displacy.render(loaded_doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (naive)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT: never look like be enough \n",
      "\n",
      "SCORES:  {'neg': 0.345, 'neu': 0.655, 'pos': 0.0, 'compound': -0.2755}\n"
     ]
    }
   ],
   "source": [
    "# Using the force of VADER for a simple sentiment analysis\n",
    "# NOTE: VADERs results are based on the words, NOT the context they appear in\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Select a random document from corpus\n",
    "random_doc = random.choice(lemmatized_corpus).strip()\n",
    "print(\"CHOSEN DOCUMENT:\", random_doc.strip(),\"\\n\")\n",
    "print(\"SCORES: \", analyzer.polarity_scores(random_doc))\n",
    "# NOTE: a detailed explaination regarding the scores and their computation can be found here: https://github.com/cjhutto/vaderSentiment (accessed: 19.12.21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: python lists maintain order, context is still intact\n",
    "# NOTE: for a more sophisticated split (e.g. \"N.Y.\"), please refer to token.text of the object the nlp() method returns \n",
    "# split() will result in a nested structure of lists inside a list -> corpus = [[\"word\", \"word\", \"anotherword\"], [\"word\"], [...]]\n",
    "corpus_tokenized = [doc.split() for doc in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "corpus_cleaned = [[token for token in doc if token not in stop_words] for doc in corpus_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some info \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:        16486\n",
      "Number of words in total:     106480\n",
      "Lexical Diversity:              0.15 % \n",
      "\n",
      "Most commonly used words across all documents:\n",
      "\n",
      "say             : 1214\n",
      "hundred         : 692\n",
      "thousand        : 622\n",
      "twenty          : 589\n",
      "year            : 527\n",
      "three           : 489\n",
      "five            : 472\n",
      "also            : 467\n",
      "make            : 464\n",
      "people          : 448\n"
     ]
    }
   ],
   "source": [
    "# building a vocabulary\n",
    "unique_words = set()\n",
    "[[unique_words.add(word) for word in doc] for doc in corpus_cleaned]\n",
    "print(\"Number of unique words:   {:10}\".format(len(unique_words)))\n",
    "# counting number of words in total\n",
    "word_count_total = sum([len(sentence) for sentence in corpus_cleaned])\n",
    "print(\"Number of words in total: {:10}\".format(word_count_total))\n",
    "print(\"Lexical Diversity: {:17.2f} %\".format(len(unique_words)/word_count_total), \"\\n\")\n",
    "\n",
    "# creating a dictionary, mapping a word with its \n",
    "# 1) frequency of occurence within the entire corpus\n",
    "# 2) number of documents the term appears in (document frequency) \n",
    "# NOTE: the dict.fromkeys() method cannot be used here, due to it would assign the same list object as value for every key\n",
    "freq_dict = {key: [0, set()] for key in unique_words}\n",
    "for idx, doc in enumerate(corpus_cleaned):\n",
    "    #print(idx, doc)\n",
    "    for word in doc:\n",
    "        # updating the term frequency\n",
    "        freq_dict[word][0] += 1\n",
    "        # length of this set is equal to the document frequency\n",
    "        freq_dict[word][1].add(idx)\n",
    "        \n",
    "print(\"Most commonly used words across all documents:\\n\")\n",
    "most_common_words = sorted(freq_dict.items(), key=lambda item: item[1][0], reverse=True)\n",
    "for element in most_common_words[:10]:\n",
    "    print(\"{0:15} : {1}\".format(element[0], element[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT:  ['pride', 'wayne', 'eight', 'fifty', 'fail', 'reduce', 'speed'] \n",
      "\n",
      "eight           :      0.5401421061459631\n",
      "fail            :      0.7999174941902797\n",
      "fifty           :      0.5715506027335373\n",
      "pride           :      0.9732064427396874\n",
      "reduce          :      0.8038316333600104\n",
      "speed           :      0.8678351651495761\n",
      "wayne           :      1.001873684948566\n"
     ]
    }
   ],
   "source": [
    "random_doc = random.choice(corpus_cleaned)\n",
    "print(\"CHOSEN DOCUMENT: \", random_doc, \"\\n\")\n",
    "# np.unique() returns a set-like copy of the chosen document, this way the original list can be referenced when using the count() method \n",
    "for word in np.unique(random_doc):\n",
    "    print(\"{0:15} :      {1}\".format(\n",
    "        word, ( \n",
    "        # TF * IDF = (number of occurrences of word / total number of words in document) * log (number of documents in corpus / number of documents that contain the word+1)\n",
    "        ( random_doc.count(word) / len(random_doc) ) * \n",
    "        np.log( len(corpus_cleaned) / (len(freq_dict[word][1])+1) )\n",
    "        )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTS:\n",
      "\n",
      "['laugh', 'different', 'certainly', 'kitchen', 'splendido', 'across', 'industry']\n",
      "['hear', 'lion', 'safari', 'park', 'noise', 'night', 'terrify'] \n",
      "\n",
      "VECTOR REPRESENTATION:\n",
      "\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1] \n",
      "\n",
      "COSINE SIMILARITY: 0.0\n"
     ]
    }
   ],
   "source": [
    "# randomly choose two documents from corpus and calculate their cosine similarity\n",
    "doc1, doc2 = random.sample(corpus_cleaned, 2)\n",
    "print(\"DOCUMENTS:\\n\")\n",
    "print(doc1)\n",
    "print(doc2,\"\\n\")\n",
    "\n",
    "# creating a vocabulary of the two documents, which will be used to vectorize the two documents \n",
    "vocabulary = set(doc1).union(set(doc2))\n",
    "v1, v2 = [], []\n",
    "for word in vocabulary:\n",
    "    if word in doc1: v1.append(1)\n",
    "    else: v1.append(0)\n",
    "    if word in doc2: v2.append(1)\n",
    "    else: v2.append(0)\n",
    "\n",
    "print(\"VECTOR REPRESENTATION:\\n\")\n",
    "print(v1)\n",
    "print(v2,\"\\n\")\n",
    "\n",
    "# cosine similarity = dot-product(v1,v2) / (absolute value(v1) * absolute value(v2)) = (v1.v2) / (||v1||.||v2||)\n",
    "dot_prod = 0\n",
    "for idx in range(len(vocabulary)):\n",
    "        dot_prod += v1[idx] * v2[idx]\n",
    "cosine_similarity = dot_prod / float((sum(v1) * sum(v2)) ** 0.5)\n",
    "print(\"COSINE SIMILARITY:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHashing (or approximating the Jaccard Similarity)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTS:\n",
      "\n",
      "the quick brown fox jumped over the lazy dog \n",
      "\n",
      "the quick brown dog jumped over the lazy fox \n",
      "\n",
      "APPROXIMATED SIMILARITY: ~ 100.0 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\" for more details see: http://infolab.stanford.edu/~ullman/mmds/ch3.pdf \"\"\"\n",
    "\n",
    "# for demonstration purpose two very similar documents are choosen\n",
    "doc1 = \"the quick brown fox jumped over the lazy dog\"\n",
    "doc2 = \"the quick brown dog jumped over the lazy fox\"\n",
    "\n",
    "# helper method to create shingles of given size (basically n-grams with words as morphems) \n",
    "# INPUT: [\"word1 word2 word3 word4\" ...], OUTPUT: [\"word1 word2 word3\", \"word2 word3 word4\", ...]\n",
    "def get_shingles(document, size = 3):\n",
    "    return ngrams(document.split(), size)\n",
    "\n",
    "# method to calculate a signature matrix from given list of hashfunctions and document vectors\n",
    "def get_signature_matrix(document_vectors: list, permutated_lists: list) -> np.array:\n",
    "    # defining a yet uninitialized signature matrix (S) of dimension S(number of hash functions Ã— number of documents)\n",
    "    # using positive infinity to represent the uninitialized state \n",
    "    signature_matrix = np.empty((len(permutated_lists), len(document_vectors)))\n",
    "    signature_matrix[:] = np.inf\n",
    "\n",
    "    for row, permutated_list in enumerate(permutated_lists):\n",
    "        # using permutated order of shingle-IDs to update signature matrix\n",
    "        for shingle_id in permutated_list:\n",
    "            # if shingle is present in given vector, initialize/update signature matrix at index corresponding to given hash-function (col) and document (row)\n",
    "            for col, doc_vec in enumerate(document_vectors):\n",
    "                if doc_vec[shingle_id] == 1:\n",
    "                    if signature_matrix[row][col] > shingle_id:\n",
    "                        signature_matrix[row][col] = shingle_id\n",
    "    return signature_matrix\n",
    "\n",
    "# use this method to calculate the approximate similarity between the documents specified by their index in the given signature matrix\n",
    "def approximate_similarity(signature_matrix: np.array, idx1: int, idx2: int) -> float:\n",
    "    # count of matching values / total number of hash-functions used for permutation (number of rows in signature matrix)\n",
    "    return len(np.where(signature_matrix[idx1] == signature_matrix[idx2])[0]) / signature_matrix.shape[0]\n",
    "\n",
    "# divide sentences into list shingles of given size\n",
    "doc1_shingles = list(get_shingles(doc1))\n",
    "doc2_shingles = list(get_shingles(doc2))\n",
    "# create a set of shingles\n",
    "set_of_shingles = set(doc1_shingles + doc2_shingles)\n",
    "# assign a unique ID to every shingle\n",
    "shingle_dict = {}\n",
    "for idx, shingle in enumerate(set_of_shingles):\n",
    "    shingle_dict[idx] = shingle\n",
    "\n",
    "# use the dictionary to represent documents as vectors (see src, p.81), \n",
    "# in combination the dictionary and vectors can be interpreted as the \"document matrix\"\n",
    "v1, v2 = [], []\n",
    "for shingle in shingle_dict.values():\n",
    "    if shingle in doc1_shingles: v1.append(1)\n",
    "    else: v1.append(0)\n",
    "    if shingle in doc2_shingles: v2.append(1)\n",
    "    else: v2.append(0)\n",
    "\n",
    "# using two hash-functions to permutate the order of shingle-IDs, \n",
    "# NOTE: make sure the modulus is a prime number greater or equal to the number of shingles within the set of shingles to avoid hash collisions\n",
    "# NOTE: the more hash-functions are used, the more accurate the approximation of the Jaccard Similarity will become \n",
    "permutated_list_1 = [(id + 1) % 11 for id in shingle_dict.keys()]\n",
    "permutated_list_2 = [(3*id + 1) % 11 for id in shingle_dict.keys()]\n",
    "\n",
    "print(\"DOCUMENTS:\\n\")\n",
    "print(doc1, \"\\n\")\n",
    "print(doc2, \"\\n\")\n",
    "print(\"APPROXIMATED SIMILARITY: ~\", \n",
    "    approximate_similarity(\n",
    "        get_signature_matrix(\n",
    "            # list of document vectors\n",
    "            [v1, v2], \n",
    "            # list of permuted shingle-IDs \n",
    "            [permutated_list_1, permutated_list_2]),\n",
    "            # indexes of documents in signature matrix that should be compared with respect to their similarity\n",
    "            0, 1\n",
    "        ) * 100, \"%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appear in context similar to 'captain':\n",
      "\n",
      "ahab\n",
      "said\n",
      "cried\n",
      "hands\n",
      "starbuck\n",
      "know\n",
      "hand\n",
      "men\n",
      "old\n",
      "well\n"
     ]
    }
   ],
   "source": [
    "# due to the nature of the previously used corpus, for the following example a more coherent dataset will be used: \"Moby Dick\" by Herman Melville\n",
    "md = gutenberg.sents('melville-moby_dick.txt')\n",
    "# dataset is already tokenized, just need to ge rid off punctuation, convert strings to lower case and remove short words and stopwords\n",
    "md_puctuation_removed = [[re.sub(\"\\W+\", \"\", word.lower()) for word in doc] for doc in md]\n",
    "md_long_words = [[word for word in doc if len(word) > 1] for doc in md_puctuation_removed]\n",
    "md_cleaned = [[word for word in sentence if word not in stop_words] for sentence in md_long_words]\n",
    "\n",
    "model = Word2Vec(\n",
    "    # use cleaned corpus for training the model\n",
    "    sentences = md_cleaned, \n",
    "    # size of the layers within the neural network\n",
    "    vector_size = 300, \n",
    "    # number of words that should be considered when modelling the context\n",
    "    window = 5, \n",
    "    # consider only words with an occurrence greater or equal to min_count\n",
    "    min_count = 3, \n",
    "    # number of threads accessible to the model during training\n",
    "    workers = 4\n",
    "    )\n",
    "\n",
    "# find words that appeared in similar contexts \n",
    "term = \"captain\"\n",
    "print(f\"Words that appear in context similar to \\'{term}\\':\\n\")   \n",
    "similars = model.wv.most_similar(term, topn = 10)\n",
    "for word in similars:\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA) aka Latent Semantic Indexing (LSI)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Dictionary (Index, Token)\n",
      "[(0, 'fleece'), (1, 'lamb'), (2, 'little'), (3, 'mary'), (4, 'snow'), (5, 'sure'), (6, 'went'), (7, 'white')] \n",
      "\n",
      "Count Vector representation of the Body (rows = document, columns = count of occurrence of Token within given document)\n",
      "[[0 1 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 1]\n",
      " [0 0 0 1 0 0 1 0]\n",
      " [0 1 0 0 0 1 0 0]] \n",
      "\n",
      "topic modelling\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary had a little lamb</td>\n",
       "      <td>1.632993e+00</td>\n",
       "      <td>4.496036e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whose fleece was white as snow</td>\n",
       "      <td>3.931749e-16</td>\n",
       "      <td>1.732051e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and everywhere that Mary went</td>\n",
       "      <td>8.164966e-01</td>\n",
       "      <td>1.320577e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the lamb was sure to go</td>\n",
       "      <td>8.164966e-01</td>\n",
       "      <td>-1.548072e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             body       topic_1       topic_2\n",
       "0          Mary had a little lamb  1.632993e+00  4.496036e-18\n",
       "1  whose fleece was white as snow  3.931749e-16  1.732051e+00\n",
       "2   and everywhere that Mary went  8.164966e-01  1.320577e-15\n",
       "3         the lamb was sure to go  8.164966e-01 -1.548072e-15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componenwise\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fleece</th>\n",
       "      <td>1.940353e-16</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lamb</th>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>-7.187998e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>4.082483e-01</td>\n",
       "      <td>5.092452e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mary</th>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>6.723713e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>-8.292722e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>6.482057e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_1       topic_2\n",
       "fleece  1.940353e-16  5.773503e-01\n",
       "lamb    6.123724e-01 -7.187998e-16\n",
       "little  4.082483e-01  5.092452e-17\n",
       "mary    6.123724e-01  6.723713e-16\n",
       "snow    9.956981e-17  5.773503e-01\n",
       "sure    2.041241e-01 -8.292722e-16\n",
       "went    2.041241e-01  6.482057e-16\n",
       "white   9.956981e-17  5.773503e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source for this part is the LSA-Series of Databricks Academy video series: youtube.com/watch?v=hB51kkus-Rc (accessed: 12.02.22)\n",
    "\"\"\"\n",
    "Used for topic modelling or comparing document similarity based on topics -> similar words appear in similar contexts\n",
    "\n",
    "Latent = hidden\n",
    "\n",
    "1) Retrieve Data\n",
    "2) Build Document-Term Matrix\n",
    "3) Perform Singular Value Decomposition (SVD) on Document-Term Matrix, representing it as the product of two matricies\n",
    "4) Examine the generated topic-encoded data\n",
    "\"\"\"\n",
    "\n",
    "# 1) naive corpus, consisting of four documents\n",
    "body = [\n",
    "    \"Mary had a little lamb\",\n",
    "    \"whose fleece was white as snow\",\n",
    "    \"and everywhere that Mary went\",\n",
    "    \"the lamb was sure to go\",\n",
    "    ]\n",
    "\n",
    "# 2) building the Document-Term Matrix, CountVectorizers default configuration also takes care of transforming words to lowercase before tokenization\n",
    "# NOTE: for larger data sets the \"min_df\" parameter can be used to regulate in how many documents a term must occur to be taken into consideration\n",
    "vectorizer  = CountVectorizer(stop_words = \"english\")\n",
    "bag_of_words = vectorizer.fit_transform(body)\n",
    "\n",
    "# display intermediate results\n",
    "features = vectorizer.get_feature_names_out() \n",
    "print(\"Corpus Dictionary (Index, Token)\")\n",
    "print(list(zip(range(len(features)), features)), os.linesep)\n",
    "print(\"Count Vector representation of the Body (rows = document, columns = count of occurrence of Token within given document)\")\n",
    "# .todense() transforms results into matrix-like representation\n",
    "print(bag_of_words.todense(), os.linesep)\n",
    "\n",
    "# 3) initialize Singular Value Decomposition Model, passing amount of topics the vectors should be reduced to (similar to PCA)\n",
    "svd = TruncatedSVD(n_components = 2)\n",
    "# topic encoded data\n",
    "lsa = svd.fit_transform(bag_of_words)\n",
    "\n",
    "# 4) display the modelled numeric relationship between the give documents\n",
    "topic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_1\", \"topic_2\"])\n",
    "topic_encoded_df[\"body\"] = body\n",
    "print(\"topic modelling\")\n",
    "display(topic_encoded_df[[\"body\", \"topic_1\", \"topic_2\"]])\n",
    "\n",
    "# display numerically modelled relationship of each token with given number of topics -> latent features\n",
    "encoding_matrix = pd.DataFrame( svd.components_,\n",
    "                                index = [\"topic_1\", \"topic_2\"],\n",
    "                                columns = features).T\n",
    "print(\"Componenwise\")                                \n",
    "display(encoding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted for Topic 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lamb</th>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>-7.187998e-16</td>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>7.187998e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mary</th>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>6.723713e-16</td>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>6.723713e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>4.082483e-01</td>\n",
       "      <td>5.092452e-17</td>\n",
       "      <td>4.082483e-01</td>\n",
       "      <td>5.092452e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>-8.292722e-16</td>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>8.292722e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>6.482057e-16</td>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>6.482057e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fleece</th>\n",
       "      <td>1.940353e-16</td>\n",
       "      <td>5.773503e-01</td>\n",
       "      <td>1.940353e-16</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_1       topic_2   abs_topic_1   abs_topic_2\n",
       "lamb    6.123724e-01 -7.187998e-16  6.123724e-01  7.187998e-16\n",
       "mary    6.123724e-01  6.723713e-16  6.123724e-01  6.723713e-16\n",
       "little  4.082483e-01  5.092452e-17  4.082483e-01  5.092452e-17\n",
       "sure    2.041241e-01 -8.292722e-16  2.041241e-01  8.292722e-16\n",
       "went    2.041241e-01  6.482057e-16  2.041241e-01  6.482057e-16\n",
       "fleece  1.940353e-16  5.773503e-01  1.940353e-16  5.773503e-01\n",
       "snow    9.956981e-17  5.773503e-01  9.956981e-17  5.773503e-01\n",
       "white   9.956981e-17  5.773503e-01  9.956981e-17  5.773503e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted for Topic 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fleece</th>\n",
       "      <td>1.940353e-16</td>\n",
       "      <td>5.773503e-01</td>\n",
       "      <td>1.940353e-16</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snow</th>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "      <td>9.956981e-17</td>\n",
       "      <td>5.773503e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>-8.292722e-16</td>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>8.292722e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lamb</th>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>-7.187998e-16</td>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>7.187998e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mary</th>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>6.723713e-16</td>\n",
       "      <td>6.123724e-01</td>\n",
       "      <td>6.723713e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>6.482057e-16</td>\n",
       "      <td>2.041241e-01</td>\n",
       "      <td>6.482057e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>4.082483e-01</td>\n",
       "      <td>5.092452e-17</td>\n",
       "      <td>4.082483e-01</td>\n",
       "      <td>5.092452e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic_1       topic_2   abs_topic_1   abs_topic_2\n",
       "fleece  1.940353e-16  5.773503e-01  1.940353e-16  5.773503e-01\n",
       "snow    9.956981e-17  5.773503e-01  9.956981e-17  5.773503e-01\n",
       "white   9.956981e-17  5.773503e-01  9.956981e-17  5.773503e-01\n",
       "sure    2.041241e-01 -8.292722e-16  2.041241e-01  8.292722e-16\n",
       "lamb    6.123724e-01 -7.187998e-16  6.123724e-01  7.187998e-16\n",
       "mary    6.123724e-01  6.723713e-16  6.123724e-01  6.723713e-16\n",
       "went    2.041241e-01  6.482057e-16  2.041241e-01  6.482057e-16\n",
       "little  4.082483e-01  5.092452e-17  4.082483e-01  5.092452e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "further analysis; find what words account for the greatest amount of variance within the data, \n",
    "NOTE: positive/negative values can be of equal importance, hence the absolute values are taken into consideration, \n",
    "the greater the distance between certain topic values the better it can be distinguished with regards to their topic-wise similarity\n",
    "\"\"\"\n",
    "encoding_matrix[\"abs_topic_1\"] = np.abs(encoding_matrix[\"topic_1\"])\n",
    "encoding_matrix[\"abs_topic_2\"] = np.abs(encoding_matrix[\"topic_2\"])\n",
    "print(\"Sorted for Topic 1\")\n",
    "display(encoding_matrix.sort_values(\"abs_topic_1\", ascending = False))\n",
    "print(\"Sorted for Topic 2\")\n",
    "display(encoding_matrix.sort_values(\"abs_topic_2\", ascending = False))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c72c38b4803b3eca2f9713ba7cb595587c467c90cd378d0f8c1e8f63267d9c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
