{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP Model for Lemmatization and Named Entity Recognition (NER)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Stopwords\"\"\"\n",
    "# nltk.download(\"<MODULE_NAME>\") e.g. \"stopwords\" or \"wordnet\"\n",
    "# src: https://pypi.org/project/nltk/\n",
    "\"\"\"Number to Word Translalator\"\"\"\n",
    "# pip install num2words\n",
    "# src: https://pypi.org/project/words2num/\n",
    "\"\"\"NLP Model for Lemmatization and Named Entity Recognition (NER)\"\"\"\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# NOTE: If pip is not available to you, please refer to https://spacy.io/usage and select an installation option suiting to your environment :)\n",
    "\"\"\"Cython - needs to be installed to make use of the 'worker' parameter in Word2Vec model (parallelization)\"\"\"\n",
    "# pip install Cython\n",
    "# src: https://pypi.org/project/Cython/\n",
    "\"\"\"Valence Aware Dictionary and sEntiment Reasoner (VADER)\"\"\"\n",
    "# pip install vaderSentiment\n",
    "# src: https://pypi.org/project/vaderSentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and loading the data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords, gutenberg\n",
    "from num2words import num2words\n",
    "from gensim.models import Word2Vec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# src: https://wortschatz.uni-leipzig.de/en/download/English#eng_news_2020 (accessed: 14.12.21)\n",
    "with open(\"eng_news_2020_10K-sentences.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    corpus_raw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the enumeration infront of each doc\n",
    "corpus_without_enum = [doc.split(\"\\t\")[1] for doc in corpus_raw]\n",
    "# converting every word to lower case\n",
    "corpus_lowercase = [doc.lower() for doc in corpus_without_enum]\n",
    "# insert whitespace between numbers and words\n",
    "corpus_words_and_numbers_separated = [re.sub(\"([a-zA-Z]*)(\\d+)([a-zA-Z]*)\", r\"\\1 \\2 \\3\", doc) for doc in corpus_lowercase]\n",
    "# convert numbers into their word equivalent while maintaining their position\n",
    "corpus_num2words = []\n",
    "for doc in corpus_words_and_numbers_separated:\n",
    "    numbers_in_doc = re.findall(\"\\d+\", doc)  \n",
    "    # returns false if list is empty\n",
    "    if numbers_in_doc:\n",
    "        for number in numbers_in_doc:\n",
    "            doc = doc.replace(number, num2words(number, lang=\"en\"))\n",
    "        corpus_num2words.append(doc)    \n",
    "    else:\n",
    "        corpus_num2words.append(doc)   \n",
    "# removing words shorter than 4 characters\n",
    "corpus_longer_words = [re.sub(r\"\\b\\w{,3}\\b\", \"\", doc) for doc in corpus_num2words]\n",
    "# removing punctuation\n",
    "corpus_no_punctuation = [re.sub(r\"(\\W+)\", \" \", doc) for doc in corpus_longer_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization, loading the small standard Model with all components \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Loop is necessary due to spaCys nlp() method does not accept iterables\n",
    "lemmatized_corpus = []\n",
    "for doc in corpus_no_punctuation:\n",
    "    loaded_doc = nlp(doc)\n",
    "    lemmatized_corpus.append(\" \".join([token.lemma_ for token in loaded_doc])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech-Tagging (POS-Tagging)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT: late nature announcement thursday make only hour before measure come into effect leave many confused \n",
      "\n",
      "TEXT: late            POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: nature          POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: announcement    POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: thursday        POS: PROPN      DETAILS: noun, proper singular\n",
      "TEXT: make            POS: VERB       DETAILS: verb, non-3rd person singular present\n",
      "TEXT: only            POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: hour            POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: before          POS: SCONJ      DETAILS: conjunction, subordinating or preposition\n",
      "TEXT: measure         POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: come            POS: VERB       DETAILS: verb, non-3rd person singular present\n",
      "TEXT: into            POS: ADP        DETAILS: conjunction, subordinating or preposition\n",
      "TEXT: effect          POS: NOUN       DETAILS: noun, singular or mass\n",
      "TEXT: leave           POS: VERB       DETAILS: verb, non-3rd person singular present\n",
      "TEXT: many            POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "TEXT: confused        POS: ADJ        DETAILS: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "VISUALIZATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"43e37e5ec77444bcab17df9efb4fdf85-0\" class=\"displacy\" width=\"2300\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #ECB365; background: #041C32; font-family: Source Sans Pro; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">late</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">nature</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">announcement</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">thursday</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">make</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">only</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">hour</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">before</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">measure</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">come</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">into</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1700\">effect</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1700\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">leave</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2000\">many</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2000\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">confused</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-0\" stroke-width=\"2px\" d=\"M62,302.0 62,277.0 191.0,277.0 191.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,304.0 L58,296.0 66,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-1\" stroke-width=\"2px\" d=\"M212,302.0 212,277.0 341.0,277.0 341.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,304.0 L208,296.0 216,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-2\" stroke-width=\"2px\" d=\"M362,302.0 362,252.0 644.0,252.0 644.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,304.0 L358,296.0 366,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-3\" stroke-width=\"2px\" d=\"M512,302.0 512,277.0 641.0,277.0 641.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M512,304.0 L508,296.0 516,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-4\" stroke-width=\"2px\" d=\"M812,302.0 812,277.0 941.0,277.0 941.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,304.0 L808,296.0 816,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-5\" stroke-width=\"2px\" d=\"M662,302.0 662,252.0 944.0,252.0 944.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M944.0,304.0 L948.0,296.0 940.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-6\" stroke-width=\"2px\" d=\"M1112,302.0 1112,252.0 1394.0,252.0 1394.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1112,304.0 L1108,296.0 1116,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-7\" stroke-width=\"2px\" d=\"M1262,302.0 1262,277.0 1391.0,277.0 1391.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1262,304.0 L1258,296.0 1266,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-8\" stroke-width=\"2px\" d=\"M662,302.0 662,202.0 1400.0,202.0 1400.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1400.0,304.0 L1404.0,296.0 1396.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-9\" stroke-width=\"2px\" d=\"M1412,302.0 1412,277.0 1541.0,277.0 1541.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1541.0,304.0 L1545.0,296.0 1537.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-10\" stroke-width=\"2px\" d=\"M1562,302.0 1562,277.0 1691.0,277.0 1691.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1691.0,304.0 L1695.0,296.0 1687.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-11\" stroke-width=\"2px\" d=\"M1412,302.0 1412,227.0 1847.0,227.0 1847.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1847.0,304.0 L1851.0,296.0 1843.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-12\" stroke-width=\"2px\" d=\"M1862,302.0 1862,277.0 1991.0,277.0 1991.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1991.0,304.0 L1995.0,296.0 1987.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-43e37e5ec77444bcab17df9efb4fdf85-0-13\" stroke-width=\"2px\" d=\"M1862,302.0 1862,252.0 2144.0,252.0 2144.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-43e37e5ec77444bcab17df9efb4fdf85-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">oprd</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2144.0,304.0 L2148.0,296.0 2140.0,296.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select a random document from corpus\n",
    "random_doc = random.choice(lemmatized_corpus)\n",
    "loaded_doc = nlp(random_doc.strip())\n",
    "\n",
    "print(\"CHOSEN DOCUMENT:\", loaded_doc, \"\\n\")\n",
    "# Use spaCy POS-Tagging\n",
    "for token in loaded_doc:\n",
    "    print(\n",
    "        f\"TEXT: {token.text:15}\",\n",
    "        f\"POS: {token.pos_:10}\", \n",
    "        f\"DETAILS: {spacy.explain(token.tag_):5}\"\n",
    "        )\n",
    "\n",
    "print(\"\\nVISUALIZATION:\")\n",
    "# specify options for visualization        \n",
    "options = {\n",
    "    \"compact\": True, \n",
    "    \"bg\": \"#041C32\",\n",
    "    \"color\": \"#ECB365\", \n",
    "    \"font\": \"Source Sans Pro\"\n",
    "    }\n",
    "# display POS and relationship of words within the sentence    \n",
    "spacy.displacy.render(loaded_doc, style=\"dep\", options=options)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entitiy Recognition (NER)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT: As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect. \n",
      "\n",
      "TEXT: Gregor Samsa    LABEL: PERSON     DETAILS: People, including fictional\n",
      "TEXT: one morning     LABEL: TIME       DETAILS: Times smaller than a day\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">As \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gregor Samsa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " awoke \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one morning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " from uneasy dreams he found himself transformed in his bed into a gigantic insect.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Due to not every doc in the corpus contains entities, a sample is provided manually\n",
    "loaded_doc = nlp(\"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\")\n",
    "\n",
    "print(\"CHOSEN DOCUMENT:\", loaded_doc, \"\\n\")\n",
    "# Use spaCy POS-Tagging\n",
    "for token in loaded_doc.ents:\n",
    "    print(\n",
    "        f\"TEXT: {token.text:15}\",\n",
    "        f\"LABEL: {token.label_:10}\", \n",
    "        f\"DETAILS: {spacy.explain(token.label_):19}\"\n",
    "        )\n",
    "\n",
    "# display POS and relationship of words within the sentence    \n",
    "spacy.displacy.render(loaded_doc, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (naive)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN DOCUMENT: participant encourage call ahead confirm open hour \n",
      "\n",
      "SCORES:  {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.5106}\n"
     ]
    }
   ],
   "source": [
    "# Using the force of VADER for a simple sentiment analysis\n",
    "# NOTE: VADERs results are based on the words, NOT the context they appear in\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Select a random document from corpus\n",
    "random_doc = random.choice(lemmatized_corpus).strip()\n",
    "print(\"CHOSEN DOCUMENT:\", random_doc.strip(),\"\\n\")\n",
    "print(\"SCORES: \", analyzer.polarity_scores(random_doc))\n",
    "# NOTE: a detailed explaination regarding the scores and their computation can be found here: https://github.com/cjhutto/vaderSentiment (accessed: 19.12.21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: python lists maintain order, context is still intact\n",
    "# NOTE: for a more sophisticated split (e.g. \"N.Y.\"), please refer to token.text of the object the nlp() method returns \n",
    "# split() will result in a nested structure of lists inside a list -> corpus = [[\"word\", \"word\", \"anotherword\"], [\"word\"], [...]]\n",
    "corpus_tokenized = [doc.split() for doc in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "corpus_cleaned = [[token for token in doc if token not in stop_words] for doc in corpus_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display some info \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:        16486\n",
      "Number of words in total:     106480\n",
      "Lexical Diversity:              0.15 % \n",
      "\n",
      "Most commonly used words across all documents:\n",
      "\n",
      "say             : 1214\n",
      "hundred         : 692\n",
      "thousand        : 622\n",
      "twenty          : 589\n",
      "year            : 527\n",
      "three           : 489\n",
      "five            : 472\n",
      "also            : 467\n",
      "make            : 464\n",
      "people          : 448\n"
     ]
    }
   ],
   "source": [
    "# building a vocabulary\n",
    "unique_words = set()\n",
    "[[unique_words.add(word) for word in doc] for doc in corpus_cleaned]\n",
    "print(\"Number of unique words:   {:10}\".format(len(unique_words)))\n",
    "# counting number of words in total\n",
    "word_count_total = sum([len(sentence) for sentence in corpus_cleaned])\n",
    "print(\"Number of words in total: {:10}\".format(word_count_total))\n",
    "print(\"Lexical Diversity: {:17.2f} %\".format(len(unique_words)/word_count_total), \"\\n\")\n",
    "\n",
    "# creating a dictionary, mapping a word with its \n",
    "# 1) frequency of occurence within the entire corpus\n",
    "# 2) number of documents the term appears in (document frequency) \n",
    "# NOTE: the dict.fromkeys() method cannot be used here, due to it would assign the same list object as value for every key\n",
    "freq_dict = {key: [0, set()] for key in unique_words}\n",
    "for idx, doc in enumerate(corpus_cleaned):\n",
    "    #print(idx, doc)\n",
    "    for word in doc:\n",
    "        # updating the term frequency\n",
    "        freq_dict[word][0] += 1\n",
    "        # length of this set is equal to the document frequency\n",
    "        freq_dict[word][1].add(idx)\n",
    "        \n",
    "print(\"Most commonly used words across all documents:\\n\")\n",
    "most_common_words = sorted(freq_dict.items(), key=lambda item: item[1][0], reverse=True)\n",
    "for element in most_common_words[:10]:\n",
    "    print(\"{0:15} : {1}\".format(element[0], element[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen document:  ['square', 'make', 'loan', 'aggressively', 'grow', 'loan', 'business', 'take', 'banking', 'license'] \n",
      "\n",
      "aggressively    :      0.8111728083308073\n",
      "banking         :      0.7600902459542083\n",
      "business        :      0.4358310108056566\n",
      "grow            :      0.48928522584398726\n",
      "license         :      0.6645391014514647\n",
      "loan            :      1.2238595837235735\n",
      "make            :      0.31033174842339284\n",
      "square          :      0.6907755278982137\n",
      "take            :      0.3272804166893757\n"
     ]
    }
   ],
   "source": [
    "random_doc = random.choice(corpus_cleaned)\n",
    "print(\"CHOSEN DOCUMENT: \",random_doc,\"\\n\")\n",
    "# np.unique() returns a set-like copy of the chosen document, this way the original list can be referenced when using the count() method \n",
    "for word in np.unique(random_doc):\n",
    "    print(\"{0:15} :      {1}\".format(\n",
    "        word, ( \n",
    "        # TF * IDF = (number of occurrences of word / total number of words in document) * log (number of documents in corpus / number of documents that contain the word+1)\n",
    "        ( random_doc.count(word) / len(random_doc) ) * \n",
    "        np.log( len(corpus_cleaned) / (len(freq_dict[word][1])+1) )\n",
    "        )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTS:\n",
      "\n",
      "['italy', 'way', 'mess', 'leave', 'euro', 'force', 'northern', 'bloc', 'uncle']\n",
      "['malta', 'international', 'airport', 'remain', 'ninety', 'level', 'across', 'zero', 'share'] \n",
      "\n",
      "VECTOR REPRESENTATION:\n",
      "\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0] \n",
      "\n",
      "COSINE SIMILARITY: 0.0\n"
     ]
    }
   ],
   "source": [
    "# randomly choose two documents from corpus and calculate their cosine similarity\n",
    "doc1, doc2 = random.sample(corpus_cleaned, 2)\n",
    "print(\"DOCUMENTS:\\n\")\n",
    "print(doc1)\n",
    "print(doc2,\"\\n\")\n",
    "\n",
    "# creating a vocabulary of the two documents, which will be used to vectorize the two documents \n",
    "vocabulary = set(doc1).union(set(doc2))\n",
    "v1, v2 = [], []\n",
    "for word in vocabulary:\n",
    "    if word in doc1: v1.append(1)\n",
    "    else: v1.append(0)\n",
    "    if word in doc2: v2.append(1)\n",
    "    else: v2.append(0)\n",
    "\n",
    "print(\"VECTOR REPRESENTATION:\\n\")\n",
    "print(v1)\n",
    "print(v2,\"\\n\")\n",
    "\n",
    "# cosine similarity = dot-product(v1,v2) / (absolute value(v1) * absolute value(v2)) = (v1.v2) / (||v1||.||v2||)\n",
    "dot_prod = 0\n",
    "for idx in range(len(vocabulary)):\n",
    "        dot_prod += v1[idx]*v2[idx]\n",
    "cosine_similarity = dot_prod / float((sum(v1)*sum(v2))**0.5)\n",
    "print(\"COSINE SIMILARITY:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appear in context similar to 'captain':\n",
      "\n",
      "ahab\n",
      "said\n",
      "cried\n",
      "starbuck\n",
      "old\n",
      "stubb\n",
      "well\n",
      "know\n",
      "thought\n",
      "tell\n"
     ]
    }
   ],
   "source": [
    "# due to the nature of the previously used corpus, for the following example a more coherent dataset will be used: \"Moby Dick\" by Herman Melville\n",
    "md = gutenberg.sents('melville-moby_dick.txt')\n",
    "# dataset is already tokenized, just need to ge rid off punctuation, convert strings to lower case and remove short words and stopwords\n",
    "md_puctuation_removed = [[re.sub(\"\\W+\", \"\", word.lower()) for word in doc] for doc in md]\n",
    "md_long_words = [[word for word in doc if len(word) > 1] for doc in md_puctuation_removed]\n",
    "md_cleaned = [[word for word in sentence if word not in stop_words] for sentence in md_long_words]\n",
    "\n",
    "model = Word2Vec(\n",
    "    # use cleaned corpus for training the model\n",
    "    sentences = md_cleaned, \n",
    "    # size of the layers within the neural network\n",
    "    vector_size = 300, \n",
    "    # number of words that should be considered when modelling the context\n",
    "    window = 5, \n",
    "    # consider only words with an occurrence greater or equal to min_count\n",
    "    min_count = 3, \n",
    "    # number of threads accessible to the model during training\n",
    "    workers = 4\n",
    "    )\n",
    "\n",
    "# find words that appeared in similar contexts \n",
    "term = \"captain\"\n",
    "print(f\"Words that appear in context similar to \\'{term}\\':\\n\")   \n",
    "similars = model.wv.most_similar(term, topn = 10)\n",
    "for word in similars:\n",
    "    print(word[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c72c38b4803b3eca2f9713ba7cb595587c467c90cd378d0f8c1e8f63267d9c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
